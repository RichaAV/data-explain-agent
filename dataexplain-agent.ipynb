{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13944795,"sourceType":"datasetVersion","datasetId":8887663}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Section 1: Captsone Project Title & Abstract\n\n# DataExplain - Automated Data Insights & Reporting Agent  \n### Google 5-Day AI Agents Intensive - Capstone Project  \n### Author: Richa Ann Varghese\n\nThis project builds a multi-agent system that automatically analyzes tabular datasets, \ngenerates insights, creates visualizations, evaluates their quality, refines them, \nand produces a clear final report.\n\nThe system demonstrates:\n- Multi-agent workflow  \n- Custom tools (CSV loader, profiler, plot generator, stats tool)  \n- Insight generation  \n- Agent evaluation  \n- Refinement loop  \n- Memory  \n- Report generation  \n","metadata":{}},{"cell_type":"markdown","source":"## Section 2: Import libraries and Utility Setup","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport io\nimport matplotlib.pyplot as plt\nimport os\nimport uuid\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:39.446470Z","iopub.execute_input":"2025-12-01T14:21:39.446782Z","iopub.status.idle":"2025-12-01T14:21:39.452118Z","shell.execute_reply.started":"2025-12-01T14:21:39.446760Z","shell.execute_reply":"2025-12-01T14:21:39.451243Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2. Import ADK components\nImport the specific components you'll need from the Agent Development Kit and the Generative AI library.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport google.generativeai as genai\nfrom google.genai import types\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\n\nprint(\"âœ… ADK components imported successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:39.452915Z","iopub.execute_input":"2025-12-01T14:21:39.453262Z","iopub.status.idle":"2025-12-01T14:21:39.467906Z","shell.execute_reply.started":"2025-12-01T14:21:39.453233Z","shell.execute_reply":"2025-12-01T14:21:39.466694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3 Configure your Gemini API Key","metadata":{}},{"cell_type":"code","source":"\n\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"âœ… Gemini API key setup complete.\")\nexcept Exception as e:\n    print(\n        f\"ðŸ”‘ Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your Kaggle secrets. Details: {e}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:39.469815Z","iopub.execute_input":"2025-12-01T14:21:39.470188Z","iopub.status.idle":"2025-12-01T14:21:39.715729Z","shell.execute_reply.started":"2025-12-01T14:21:39.470155Z","shell.execute_reply":"2025-12-01T14:21:39.714692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.4 Create a Gemini Model Object","metadata":{}},{"cell_type":"code","source":"# Create the Gemini LLM object used by all agents\ngemini_model = Gemini(model_name=\"gemini-1.5-flash\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:39.716802Z","iopub.execute_input":"2025-12-01T14:21:39.717272Z","iopub.status.idle":"2025-12-01T14:21:39.722230Z","shell.execute_reply.started":"2025-12-01T14:21:39.717245Z","shell.execute_reply":"2025-12-01T14:21:39.721134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\n\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n\ngpt = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n\n# Simple helper function to call Gemini and return text\ndef call_llm(prompt: str):\n    response = gpt.generate_content(prompt)\n    return response.text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:39.723249Z","iopub.execute_input":"2025-12-01T14:21:39.723546Z","iopub.status.idle":"2025-12-01T14:21:39.743562Z","shell.execute_reply.started":"2025-12-01T14:21:39.723519Z","shell.execute_reply":"2025-12-01T14:21:39.742511Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.5 Helper function","metadata":{}},{"cell_type":"code","source":"call_llm(\"Say hello in one short sentence.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:39.744827Z","iopub.execute_input":"2025-12-01T14:21:39.745280Z","iopub.status.idle":"2025-12-01T14:21:40.383250Z","shell.execute_reply.started":"2025-12-01T14:21:39.745243Z","shell.execute_reply":"2025-12-01T14:21:40.382356Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 3: Building Custom Function Tools\n\nBuilding tools to load data, profile it, generate visualizations, and compute statistics.","metadata":{}},{"cell_type":"markdown","source":"### 3.1 load_csv_tool \n\nThis tool loads the CSV file and extracts basic metadata such as column names, data types, and a small preview.\nIt helps the agents understand the structure of the dataset and understanding how the data looks like before running deeper analysis.","metadata":{}},{"cell_type":"code","source":"def load_csv_tool(file_bytes: bytes) -> dict:\n    \"\"\"\n    Tool: Load CSV file and return dataframe metadata.\n    \"\"\"\n    try:\n        # Load the CSV into a DataFrame\n        df = pd.read_csv(io.BytesIO(file_bytes))\n\n        # Extract basic dataset metadata\n        metadata = {\n            \"columns\": list(df.columns),\n            \"num_rows\": len(df),\n            \"num_columns\": df.shape[1],\n            \"column_types\": {col: str(df[col].dtype) for col in df.columns}\n        }\n\n        # Return preview and metadata\n        return {\n            \"status\": \"success\",\n            \"metadata\": metadata,\n            \"preview\": df.head(5).to_dict(orient=\"records\")\n        }\n\n    except Exception as e:\n        # Return error if loading fails\n        return {\"status\": \"error\", \"error\": str(e)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.385734Z","iopub.execute_input":"2025-12-01T14:21:40.385996Z","iopub.status.idle":"2025-12-01T14:21:40.392156Z","shell.execute_reply.started":"2025-12-01T14:21:40.385976Z","shell.execute_reply":"2025-12-01T14:21:40.391346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 profile_dataframe_tool\n\nThis tool computes summary statistics for each column, such as missing percentage, unique values, and basic numeric stats.\nIt tells us about data quality, potential issues, and overall structure, which helps downstream agents generate meaningful insights.","metadata":{}},{"cell_type":"code","source":"def profile_dataframe_tool(file_bytes: bytes) -> dict:\n    \"\"\"\n    Tool: Profile dataset to compute summary statistics.\n    \"\"\"\n    try:\n        # Load dataset\n        df = pd.read_csv(io.BytesIO(file_bytes))\n        profile = {}\n\n        # Compute statistics for each column\n        for col in df.columns:\n            col_data = df[col]\n\n            # Basic stats applicable to all columns\n            profile[col] = {\n                \"dtype\": str(col_data.dtype),\n                \"missing_pct\": float(col_data.isna().mean()),\n                \"unique_count\": int(col_data.nunique()),\n            }\n\n            # Add numeric statistics\n            if col_data.dtype != \"object\":\n                profile[col].update({\n                    \"mean\": float(col_data.mean()) if col_data.notna().any() else None,\n                    \"std\": float(col_data.std()) if col_data.notna().any() else None,\n                    \"min\": float(col_data.min()) if col_data.notna().any() else None,\n                    \"max\": float(col_data.max()) if col_data.notna().any() else None,\n                })\n\n        # Return the full profile\n        return {\"status\": \"success\", \"profile\": profile}\n\n    except Exception as e:\n        # Error handling\n        return {\"status\": \"error\", \"error\": str(e)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.392871Z","iopub.execute_input":"2025-12-01T14:21:40.393095Z","iopub.status.idle":"2025-12-01T14:21:40.408537Z","shell.execute_reply.started":"2025-12-01T14:21:40.393068Z","shell.execute_reply":"2025-12-01T14:21:40.407494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3 generate_plots_tool\nThis tool creates visualizations for both numeric and categorical columns.\nIt generates histograms, boxplots, bar charts, and a correlation heatmap.\nThese plots help highlight distribution patterns, outliers, category frequencies, and relationships between numeric variables.\nVisuals are saved as image files so that other agents (e.g., ReportAgent) can include them in the final output.","metadata":{}},{"cell_type":"code","source":"# Tool: Generate grouped plots (histograms, boxplots, barplots, heatmap)\n# and return both file paths + automated insights.\ndef generate_plots_tool(file_bytes: bytes):\n    try:\n        df = pd.read_csv(io.BytesIO(file_bytes))\n\n        # Create plots folder if not exists\n        os.makedirs(\"plots\", exist_ok=True)\n\n        numeric_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n        categorical_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n\n        # Storage for results\n        results = {\n            \"histograms\": [],\n            \"boxplots\": [],\n            \"barplots\": [],\n            \"heatmap\": None,\n            \"insights\": {\n                \"histogram_insights\": [],\n                \"boxplot_insights\": [],\n                \"barplot_insights\": [],\n                \"correlation_insights\": []\n            }\n        }\n\n        # ------------------------------\n        # HISTOGRAMS + Insights\n        # ------------------------------\n        for col in numeric_cols:\n            plt.figure()\n            df[col].plot(kind=\"hist\", bins=30, edgecolor='black')\n            plt.title(f\"Histogram of {col}\")\n            file_path = f\"plots/{col}_hist.png\"\n            plt.savefig(file_path)\n            plt.close()\n\n            # Automated histogram insight (skewness)\n            skew = df[col].skew()\n            if skew > 1:\n                insight = f\"{col} is highly right-skewed.\"\n            elif skew < -1:\n                insight = f\"{col} is highly left-skewed.\"\n            else:\n                insight = f\"{col} distribution is approximately symmetric.\"\n\n            results[\"histograms\"].append({\"column\": col, \"file\": file_path})\n            results[\"insights\"][\"histogram_insights\"].append(insight)\n\n        # ------------------------------\n        # BOX PLOTS + Outlier Insights\n        # ------------------------------\n        for col in numeric_cols:\n            plt.figure()\n            df[col].plot(kind=\"box\")\n            plt.title(f\"Box plot of {col}\")\n            file_path = f\"plots/{col}_box.png\"\n            plt.savefig(file_path)\n            plt.close()\n\n            # Automated outlier detection (IQR)\n            Q1 = df[col].quantile(0.25)\n            Q3 = df[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower = Q1 - 1.5 * IQR\n            upper = Q3 + 1.5 * IQR\n            outliers = df[(df[col] < lower) | (df[col] > upper)][col].count()\n\n            if outliers > 0:\n                insight = f\"{col} contains {outliers} potential outliers.\"\n            else:\n                insight = f\"{col} shows no significant outliers.\"\n\n            results[\"boxplots\"].append({\"column\": col, \"file\": file_path})\n            results[\"insights\"][\"boxplot_insights\"].append(insight)\n\n        # ------------------------------\n        # BAR PLOTS (categorical) + Class Balance Insights\n        # ------------------------------\n        for col in categorical_cols:\n            plt.figure()\n            df[col].value_counts().plot(kind=\"bar\")\n            plt.title(f\"Bar plot of {col}\")\n            file_path = f\"plots/{col}_bar.png\"\n            plt.savefig(file_path)\n            plt.close()\n\n            # Class balance insight\n            value_counts = df[col].value_counts(normalize=True)\n            if value_counts.max() > 0.6:\n                insight = f\"{col} is imbalanced (dominant class = {value_counts.idxmax()}).\"\n            else:\n                insight = f\"{col} is relatively balanced.\"\n\n            results[\"barplots\"].append({\"column\": col, \"file\": file_path})\n            results[\"insights\"][\"barplot_insights\"].append(insight)\n\n        # ------------------------------\n        # CORRELATION HEATMAP + Correlation Insights\n        # ------------------------------\n        if len(numeric_cols) > 1:\n            plt.figure(figsize=(8, 6))\n            corr = df[numeric_cols].corr()\n        \n            # Use imshow but manually set tick labels\n            plt.imshow(corr, cmap='coolwarm', interpolation='nearest')\n            plt.colorbar()\n        \n            # Set title\n            plt.title(\"Correlation Heatmap\")\n        \n            # Set axis ticks\n            plt.xticks(ticks=range(len(numeric_cols)), labels=numeric_cols, rotation=45, ha=\"right\")\n            plt.yticks(ticks=range(len(numeric_cols)), labels=numeric_cols)\n        \n            # Tight layout to avoid clipping\n            plt.tight_layout()\n        \n            file_path = \"plots/correlation_heatmap.png\"\n            plt.savefig(file_path)\n            plt.close()\n        \n            results[\"heatmap\"] = file_path\n        \n            # Identify high correlations\n            corr_pairs = corr.abs().unstack()\n            corr_pairs = corr_pairs[corr_pairs < 1].sort_values(ascending=False)\n            strong_corr = corr_pairs[corr_pairs > 0.75]\n        \n            if strong_corr.empty:\n                results[\"insights\"][\"correlation_insights\"].append(\"No strong correlations detected.\")\n            else:\n                for idx, val in strong_corr.items():\n                    results[\"insights\"][\"correlation_insights\"].append(\n                        f\"Strong correlation: {idx[0]} and {idx[1]} (corr = {round(val,2)})\"\n                    )\n        return {\"status\": \"success\", \"results\": results}\n\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.409578Z","iopub.execute_input":"2025-12-01T14:21:40.409893Z","iopub.status.idle":"2025-12-01T14:21:40.432810Z","shell.execute_reply.started":"2025-12-01T14:21:40.409859Z","shell.execute_reply":"2025-12-01T14:21:40.431953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.4 basic_stats_tool\nThis tool computes simple statistics (mean, median, missing %, unique count) for a single column.\nIt helps agents answer follow-up questions like â€œWhat is the average income?â€ or â€œHow many unique cities are there?â€\nUseful for the future â€œAsk my dataâ€ feature.","metadata":{}},{"cell_type":"code","source":"def basic_stats_tool(file_bytes: bytes, column: str) -> dict:\n    \"\"\"\n    Tool: Compute simple statistics for one column.\n    \"\"\"\n    try:\n        # Load dataset\n        df = pd.read_csv(io.BytesIO(file_bytes))\n\n        # Validate that column exists\n        if column not in df.columns:\n            return {\"status\": \"error\", \"error\": \"Column not found\"}\n\n        col = df[column]\n\n        # Compute basic statistics\n        stats = {\n            \"mean\": float(col.mean()) if col.dtype != \"object\" else None,\n            \"median\": float(col.median()) if col.dtype != \"object\" else None,\n            \"missing_pct\": float(col.isna().mean()),\n            \"unique_count\": int(col.nunique())\n        }\n\n        # Return stats for agent use\n        return {\"status\": \"success\", \"stats\": stats}\n\n    except Exception as e:\n        # Handle exceptional cases\n        return {\"status\": \"error\", \"error\": str(e)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.433672Z","iopub.execute_input":"2025-12-01T14:21:40.433971Z","iopub.status.idle":"2025-12-01T14:21:40.454328Z","shell.execute_reply.started":"2025-12-01T14:21:40.433942Z","shell.execute_reply":"2025-12-01T14:21:40.453303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 4: Session & Memory\nSession stores all intermediate results during the workflow.\nMemory stores long-term user preferences.","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Session State\nWe store all information from the agent workflow (metadata, profile, plots, insights, evaluations, report) inside a shared dictionary.\nEach agent reads from and writes to this dictionary.\n","metadata":{}},{"cell_type":"code","source":"# Session state stores all information the agents produce.\n# Agents update this dictionary as they work through the data.\ndef create_empty_session():\n    return {\n        \"user_id\": None,\n        \"user_goal\": None,\n        \"metadata\": None,\n        \"profile\": None,\n        \"plots\": [],\n        \"raw_insights\": None,\n        \"refined_insights\": None,\n        \"evaluation_scores\": None,\n        \"report_markdown\": None\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.455406Z","iopub.execute_input":"2025-12-01T14:21:40.455782Z","iopub.status.idle":"2025-12-01T14:21:40.474514Z","shell.execute_reply.started":"2025-12-01T14:21:40.455748Z","shell.execute_reply":"2025-12-01T14:21:40.473396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2 In-Memory Session Service\nManages all sessions in memory.","metadata":{}},{"cell_type":"code","source":"sessions = {}\n\ndef create_session(session_id: str):\n    sessions[session_id] = create_empty_session()\n    return sessions[session_id]\n\ndef get_session(session_id: str):\n    return sessions.get(session_id)\n\ndef update_session(session_id: str, key: str, value):\n    if session_id in sessions:\n        sessions[session_id][key] = value\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.475456Z","iopub.execute_input":"2025-12-01T14:21:40.475802Z","iopub.status.idle":"2025-12-01T14:21:40.490980Z","shell.execute_reply.started":"2025-12-01T14:21:40.475780Z","shell.execute_reply":"2025-12-01T14:21:40.490091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3 Memory Bank\nStores long-term user preferences such as preferred detail level or preferred visual styles.","metadata":{}},{"cell_type":"code","source":"memory_bank = {}\n\ndef get_user_memory(user_id: str):\n    return memory_bank.get(user_id, {})\n\ndef update_user_memory(user_id: str, new_preferences: dict):\n    if user_id not in memory_bank:\n        memory_bank[user_id] = {}\n    memory_bank[user_id].update(new_preferences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.491996Z","iopub.execute_input":"2025-12-01T14:21:40.492280Z","iopub.status.idle":"2025-12-01T14:21:40.500763Z","shell.execute_reply.started":"2025-12-01T14:21:40.492241Z","shell.execute_reply":"2025-12-01T14:21:40.499939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.4 Context Compaction\nReduces the size of the session before sending it to the LLM.","metadata":{}},{"cell_type":"code","source":"def compact_context(session: dict):\n    return {\n        \"user_goal\": session.get(\"user_goal\"),\n        \"metadata\": session.get(\"metadata\"),\n        \"key_insights\": session.get(\"refined_insights\"),\n        \"profile_overview\": (\n            {col: session[\"profile\"][col][\"dtype\"] for col in session[\"profile\"]}\n            if session.get(\"profile\") else None\n        )\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.501597Z","iopub.execute_input":"2025-12-01T14:21:40.502355Z","iopub.status.idle":"2025-12-01T14:21:40.517956Z","shell.execute_reply.started":"2025-12-01T14:21:40.502331Z","shell.execute_reply":"2025-12-01T14:21:40.517012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 5: Building Core Agents\nThese agents handle profiling, visualization, and insight generation.","metadata":{}},{"cell_type":"markdown","source":"### 5.1 OrchestratorAgent\n\nThis coordinates the full workflow.\nIt reads the user goal, decides which steps to run, calls the profiler, the visualization agent, and the insight generator, and stores all results in the session.\nIt acts as the controller that makes sure all agents run in the correct order.","metadata":{}},{"cell_type":"code","source":"def orchestrator_agent(session: dict, file_bytes: bytes):\n    \"\"\"\n    Controls the sequence of steps in the analysis workflow.\n    \"\"\"\n\n    # Store basic dataset metadata\n    metadata_result = load_csv_tool(file_bytes)\n    session[\"metadata\"] = metadata_result.get(\"metadata\")\n\n    # Run the profiling agent\n    profile_result = data_profiler_agent(session, file_bytes)\n    session[\"profile\"] = profile_result.get(\"profile\")\n\n    # Run the visualization agent\n    plot_result = visualization_agent(session, file_bytes)\n    session[\"plots\"] = plot_result.get(\"plots\")\n\n    # Generate first-pass insights\n    insights = insight_agent(session)\n    session[\"raw_insights\"] = insights\n\n    return session\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.518948Z","iopub.execute_input":"2025-12-01T14:21:40.519229Z","iopub.status.idle":"2025-12-01T14:21:40.539441Z","shell.execute_reply.started":"2025-12-01T14:21:40.519171Z","shell.execute_reply":"2025-12-01T14:21:40.538422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2 DataProfilerAgent\nThis agent uses the profiling tool to compute statistics such as missing values, unique counts, and numeric summaries.\nThe output helps all other agents understand the dataset better and produce meaningful insights.","metadata":{}},{"cell_type":"code","source":"def data_profiler_agent(session: dict, file_bytes: bytes):\n    \"\"\"\n    Runs the profiling tool and returns dataset statistics.\n    \"\"\"\n\n    # Run the profiling tool\n    profile_output = profile_dataframe_tool(file_bytes)\n\n    # Return only the profile dictionary\n    return {\"profile\": profile_output.get(\"profile\")}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.540419Z","iopub.execute_input":"2025-12-01T14:21:40.540689Z","iopub.status.idle":"2025-12-01T14:21:40.555071Z","shell.execute_reply.started":"2025-12-01T14:21:40.540665Z","shell.execute_reply":"2025-12-01T14:21:40.554319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.3 VisualizationAgent\nThis agent creates visual charts such as histograms, boxplots, bar charts, and a correlation heatmap.\nVisuals help highlight trends, outliers, and category distributions, and are later included in the final report","metadata":{}},{"cell_type":"code","source":"def visualization_agent(session: dict, file_bytes: bytes):\n    \"\"\"\n    Generates plots for numeric and categorical columns.\n    \"\"\"\n\n    # Run the plot-generation tool\n    plot_output = generate_plots_tool(file_bytes)\n\n    # Return list of generated plot files\n    return {\"plots\": plot_output.get(\"plots\")}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.555957Z","iopub.execute_input":"2025-12-01T14:21:40.556239Z","iopub.status.idle":"2025-12-01T14:21:40.569878Z","shell.execute_reply.started":"2025-12-01T14:21:40.556219Z","shell.execute_reply":"2025-12-01T14:21:40.568980Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.4 InsightAgent\n\nThis agent reads the dataset metadata, profile, plots, and user goal to generate human-readable insights.\nIt explains important patterns, highlights potential issues, and summarizes what the data reveals.","metadata":{}},{"cell_type":"code","source":"def insight_agent(session: dict):\n    \"\"\"\n    Generates first-pass insights using the dataset summary and user goal.\n    \"\"\"\n\n    # Prepare compact context for the LLM\n    context = {\n        \"user_goal\": session.get(\"user_goal\"),\n        \"metadata\": session.get(\"metadata\"),\n        \"profile\": session.get(\"profile\")\n    }\n\n    # Convert session into a readable prompt\n    prompt = (\n        \"You are a data analyst.\\n\"\n        \"Here is the dataset information:\\n\"\n        f\"{context}\\n\"\n        \"Generate clear and helpful insights about the dataset.\"\n    )\n##  \n    # Call the LLM to create insights\n    raw_insights = call_llm(prompt)\n\n    # Return raw insights before evaluation/refinement\n    return raw_insights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.573929Z","iopub.execute_input":"2025-12-01T14:21:40.574515Z","iopub.status.idle":"2025-12-01T14:21:40.585812Z","shell.execute_reply.started":"2025-12-01T14:21:40.574492Z","shell.execute_reply":"2025-12-01T14:21:40.584950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.5 CriticAgent\nThis agent evaluates the quality of the insights produced by the InsightAgent.\nIt scores each insight for clarity, correctness, and usefulness.\nIt also provides feedback that can be used to improve the insights.\nThis evaluation step helps ensure the final report is accurate and easy to understand.","metadata":{}},{"cell_type":"code","source":"def critic_agent(session: dict):\n    \"\"\"\n    Evaluates the insights and provides scores and feedback.\n    \"\"\"\n\n    # Prepare content to evaluate\n    insights = session.get(\"raw_insights\")\n\n    # LLM prompt to evaluate insight quality\n    prompt = (\n        \"Evaluate the following insights using 3 criteria:\\n\"\n        \"1. Clarity (0-5)\\n\"\n        \"2. Correctness (0-5)\\n\"\n        \"3. Usefulness (0-5)\\n\\n\"\n        f\"Insights:\\n{insights}\\n\\n\"\n        \"Provide a JSON response with scores and a short explanation.\"\n    )\n\n    # Get evaluation from the LLM\n    evaluation = call_llm(prompt)\n\n    # Store evaluation scores in the session\n    session[\"evaluation_scores\"] = evaluation\n\n    return evaluation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.587711Z","iopub.execute_input":"2025-12-01T14:21:40.587940Z","iopub.status.idle":"2025-12-01T14:21:40.605176Z","shell.execute_reply.started":"2025-12-01T14:21:40.587923Z","shell.execute_reply":"2025-12-01T14:21:40.603887Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.6 RefinementAgent\nThis agent improves the initial insights using the feedback from the CriticAgent.\nIf the scores are low, the agent asks the LLM to rewrite and improve the insights.\nThis creates a simple refinement loop that helps produce high-quality results.","metadata":{}},{"cell_type":"code","source":"def refinement_agent(session: dict):\n    \"\"\"\n    Improves insights based on feedback from the CriticAgent.\n    \"\"\"\n\n    # Retrieve raw insights and evaluation scores\n    raw_insights = session.get(\"raw_insights\")\n    evaluation = session.get(\"evaluation_scores\")\n\n    # Create a prompt that asks the LLM to improve the insights\n    prompt = (\n        \"Here are the initial insights:\\n\"\n        f\"{raw_insights}\\n\\n\"\n        \"Here is the evaluation of these insights:\\n\"\n        f\"{evaluation}\\n\\n\"\n        \"Rewrite and improve the insights based on the feedback.\"\n    )\n\n    # Call the LLM to refine insights\n    refined_insights = call_llm(prompt)\n\n    # Store improved insights in the session\n    session[\"refined_insights\"] = refined_insights\n\n    return refined_insights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.606238Z","iopub.execute_input":"2025-12-01T14:21:40.606567Z","iopub.status.idle":"2025-12-01T14:21:40.620972Z","shell.execute_reply.started":"2025-12-01T14:21:40.606533Z","shell.execute_reply":"2025-12-01T14:21:40.620031Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.7 ReportAgent\nThis agent creates the final analysis report.\nIt combines the dataset profile, the refined insights, and the generated visualizations into a clear markdown summary.\nThe final report is what the user receives as the final delivered output.","metadata":{}},{"cell_type":"code","source":"def report_agent(session: dict):\n    \"\"\"\n    Generates a final markdown report combining all analysis steps.\n    \"\"\"\n\n    # Collect components for the report\n    metadata = session.get(\"metadata\")\n    profile = session.get(\"profile\")\n    insights = session.get(\"refined_insights\")\n    plots = session.get(\"plots\")\n\n    # Build a simple markdown report\n    report = \"# DataExplain Report\\n\\n\"\n\n    # Add basic metadata\n    report += \"## Dataset Overview\\n\"\n    report += f\"- Rows: {metadata.get('num_rows')}\\n\"\n    report += f\"- Columns: {metadata.get('num_columns')}\\n\\n\"\n\n    # Add insights\n    report += \"## Key Insights\\n\"\n    report += f\"{insights}\\n\\n\"\n\n    # Add plots section\n    report += \"## Visualizations\\n\"\n    for p in plots:\n        report += f\"- **{p['description']}** â†’ {p['path']}\\n\"\n\n    # Save report into the session\n    session[\"report_markdown\"] = report\n\n    return report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.621893Z","iopub.execute_input":"2025-12-01T14:21:40.622172Z","iopub.status.idle":"2025-12-01T14:21:40.643136Z","shell.execute_reply.started":"2025-12-01T14:21:40.622139Z","shell.execute_reply":"2025-12-01T14:21:40.642185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.8 AskData\nThis agent answers a natural-language question using basic_stats_tool.","metadata":{}},{"cell_type":"code","source":"def ask_data(question: str, file_bytes: bytes):\n    \"\"\"\n    Two-step Ask-My-Data:\n    Step 1: Identify column\n    Step 2: Identify statistic\n    \"\"\"\n\n    df = pd.read_csv(io.BytesIO(file_bytes))\n    columns = df.columns.tolist()\n\n    # ---------------- Step 1: Identify Column ----------------\n    col_prompt = f\"\"\"\nUser question: \"{question}\"\n\nDataset columns: {columns}\n\nWhich ONE column is referenced in the question?\nReturn ONLY the column name.\nNo JSON, no extra text.\n\"\"\"\n    col_answer = call_llm(col_prompt).strip()\n\n    # Clean mapping of column answer\n    column = None\n    for c in columns:\n        if c.lower() in col_answer.lower():\n            column = c\n            break\n\n    if column is None:\n        return \"Could not interpret the question.\"\n\n    # ---------------- Step 2: Identify Statistic ----------------\n    stat_prompt = f\"\"\"\nUser question: \"{question}\"\n\nAllowed statistics:\n- mean\n- median\n- missing_pct\n- unique_count\n\nWhich statistic is the user requesting?\nReturn ONLY one of the above words.\n\"\"\"\n    stat = call_llm(stat_prompt).strip().lower()\n\n    # map synonyms\n    if \"average\" in stat:\n        stat = \"mean\"\n\n    if stat not in [\"mean\", \"median\", \"missing_pct\", \"unique_count\"]:\n        return \"Could not determine which statistic is requested.\"\n\n    # ---------------- Step 3: Compute stats ----------------\n    stats_result = basic_stats_tool(file_bytes, column)\n\n    if stats_result[\"status\"] == \"error\":\n        return stats_result[\"error\"]\n\n    value = stats_result[\"stats\"].get(stat)\n    if value is None:\n        return f\"Statistic '{stat}' unavailable.\"\n\n    return f\"The {stat} of '{column}' is {value}.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:35:13.505243Z","iopub.execute_input":"2025-12-01T14:35:13.506169Z","iopub.status.idle":"2025-12-01T14:35:13.513844Z","shell.execute_reply.started":"2025-12-01T14:35:13.506036Z","shell.execute_reply":"2025-12-01T14:35:13.512868Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 6: Final Runner Pipeline\nThis function runs the full DataExplain workflow from start to finish.\nIt loads the dataset, initializes a session, runs all agents in sequence\n(profiling, visualization, insight generation, evaluation, refinement),\nand finally creates the complete markdown report.\nThis is the main entry point for the project and will be called in the notebook demo.","metadata":{}},{"cell_type":"code","source":"def run_data_explain(file_bytes, user_goal, user_id, user_questions=None):\n\n    # Create session object\n    session_id = str(uuid.uuid4())\n    session = {\n        \"session_id\": session_id,\n        \"user_id\": user_id,\n        \"goal\": user_goal,\n        \"conversation\": []\n    }\n\n    # -------------------------------------------------------\n    # STEP 1 â€” Load CSV\n    # -------------------------------------------------------\n    load_result = load_csv_tool(file_bytes)\n    if load_result[\"status\"] == \"error\":\n        return load_result\n\n    session[\"metadata\"] = load_result[\"metadata\"]\n    session[\"preview\"] = load_result[\"preview\"]\n\n    # -------------------------------------------------------\n    # STEP 2 â€” Generate grouped plots + automated insights\n    # -------------------------------------------------------\n    plot_result = generate_plots_tool(file_bytes)\n    if plot_result[\"status\"] == \"error\":\n        return plot_result\n\n    session[\"plots\"] = plot_result[\"results\"]\n\n    # -------------------------------------------------------\n    # STEP 3 â€” Combine automated visual insights\n    # -------------------------------------------------------\n    auto_insights_block = \"## Automated Visual Insights\\n\"\n\n    auto_insights_block += \"\\n### Histogram Insights\\n\"\n    for i in session[\"plots\"][\"insights\"][\"histogram_insights\"]:\n        auto_insights_block += f\"- {i}\\n\"\n\n    auto_insights_block += \"\\n### Box Plot Insights\\n\"\n    for i in session[\"plots\"][\"insights\"][\"boxplot_insights\"]:\n        auto_insights_block += f\"- {i}\\n\"\n\n    auto_insights_block += \"\\n### Bar Plot Insights\\n\"\n    for i in session[\"plots\"][\"insights\"][\"barplot_insights\"]:\n        auto_insights_block += f\"- {i}\\n\"\n\n    auto_insights_block += \"\\n### Correlation Insights\\n\"\n    for i in session[\"plots\"][\"insights\"][\"correlation_insights\"]:\n        auto_insights_block += f\"- {i}\\n\"\n\n    session[\"auto_visual_insights\"] = auto_insights_block\n\n    # -------------------------------------------------------\n    # STEP 4 â€” Initial LLM Insight Generation\n    # -------------------------------------------------------\n    insight_prompt = f\"\"\"\nYou are a data analyst. \nThe user wants to: {user_goal}\n\nDataset metadata:\n{session[\"metadata\"]}\n\nHere is a 5-row preview:\n{session[\"preview\"]}\n\nAutomatically generated visual analysis insights:\n{auto_insights_block}\n\nProvide clear, structured insights.\n\"\"\"\n\n    raw_insights = call_llm(insight_prompt)\n    session[\"raw_insights\"] = raw_insights\n\n    # -------------------------------------------------------\n    # STEP 5 â€” Critic Agent Evaluation\n    # -------------------------------------------------------\n    critic_prompt = f\"\"\"\nEvaluate the following insights and score them (1â€“10) on:\n\n- clarity\n- correctness\n- usefulness\n\nReturn JSON ONLY:\n{{\n \"clarity\": \"\",\n \"correctness\": \"\",\n \"usefulness\": \"\",\n \"improvements\": \"\"\n}}\n\nInsights:\n{raw_insights}\n\"\"\"\n\n    critic_output = call_llm(critic_prompt)\n    session[\"critic\"] = critic_output\n\n    # -------------------------------------------------------\n    # STEP 6 â€” Refinement Agent (fixes NameError)\n    # -------------------------------------------------------\n    refine_prompt = f\"\"\"\nImprove the insights below using the critic feedback.\n\nReturn ONLY the improved insights (no explanation, no JSON):\n\nOriginal insights:\n{raw_insights}\n\nCritic feedback:\n{critic_output}\n\"\"\"\n\n    refined_insights = call_llm(refine_prompt)\n    session[\"refined_insights\"] = refined_insights\n\n    # -------------------------------------------------------\n    # STEP 7 â€” Recommended Next Steps Section\n    # -------------------------------------------------------\n    next_steps_prompt = f\"\"\"\nYou are a senior data scientist.\n\nBased on:\n- metadata\n- refined insights\n- automated visual insights\n\nSuggest 5â€“7 very practical next steps (cleaning, modeling, EDA, feature engineering, etc.).\n\nReturn only a numbered list.\n\nMetadata:\n{session['metadata']}\n\nRefined insights:\n{refined_insights}\n\nVisual insights:\n{auto_insights_block}\n\"\"\"\n\n    next_steps = call_llm(next_steps_prompt)\n    session[\"next_steps\"] = next_steps\n\n    # -------------------------------------------------------\n    # STEP 8 â€” Build Final Report\n    # -------------------------------------------------------\n    report = f\"\"\"\n# DataExplain Report\n\n## Dataset Overview\n- Rows: {session['metadata']['num_rows']}\n- Columns: {session['metadata']['num_columns']}\n\n## Key Insights (Refined)\n{refined_insights}\n\n---\n\n{auto_insights_block}\n\n---\n\n## Recommended Next Steps\n{next_steps}\n\n---\n\n## Visualizations\n\n### Histograms\n\"\"\"\n    for item in session[\"plots\"][\"histograms\"]:\n        report += f\"- **{item['column']}** â†’ {item['file']}\\n\"\n\n    report += \"\\n### Box Plots\\n\"\n    for item in session[\"plots\"][\"boxplots\"]:\n        report += f\"- **{item['column']}** â†’ {item['file']}\\n\"\n\n    report += \"\\n### Bar Plots\\n\"\n    for item in session[\"plots\"][\"barplots\"]:\n        report += f\"- **{item['column']}** â†’ {item['file']}\\n\"\n\n    report += \"\\n### Correlation Heatmap\\n\"\n    if session[\"plots\"][\"heatmap\"]:\n        report += f\"- Heatmap â†’ {session['plots']['heatmap']}\\n\"\n    else:\n        report += \"- No numeric features available.\\n\"\n\n    session[\"final_report\"] = report\n\n    \n        # -------------------------------------------------------\n    #  OPTIONAL SECTION: Answer Natural-Language Questions\n    # -------------------------------------------------------\n    if user_questions is not None:\n        if isinstance(user_questions, str):\n            user_questions = [user_questions]\n\n        qa_results = []\n        for q in user_questions:\n            answer = ask_data(q, file_bytes)\n            qa_results.append({\"question\": q, \"answer\": answer})\n\n        session[\"qa\"] = qa_results\n\n        # Append Q&A section to the final report\n        qa_md = \"\\n\\n## Ask-My-Data: Natural Language Answers\\n\"\n        for item in qa_results:\n            qa_md += f\"**Q:** {item['question']}  \\n\"\n            qa_md += f\"**A:** {item['answer']}  \\n\\n\"\n\n        session[\"final_report\"] += qa_md\n\n    \n    \n    return session\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:35:19.065277Z","iopub.execute_input":"2025-12-01T14:35:19.065589Z","iopub.status.idle":"2025-12-01T14:35:19.084392Z","shell.execute_reply.started":"2025-12-01T14:35:19.065556Z","shell.execute_reply":"2025-12-01T14:35:19.083413Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 7: Demo Dataset Upload\nUpload the test dataset using the Kaggle file upload widget.","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Upload file manually in Kaggle\nfile_path = \"/kaggle/input/test-data/data.csv\"   # update after upload\n\nwith open(file_path, \"rb\") as f:\n    file_bytes = f.read()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.682012Z","iopub.execute_input":"2025-12-01T14:21:40.682314Z","iopub.status.idle":"2025-12-01T14:21:40.702022Z","shell.execute_reply.started":"2025-12-01T14:21:40.682284Z","shell.execute_reply":"2025-12-01T14:21:40.701129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 8: Run the Full Pipeline\nSet the analysis goal and run the agent system.","metadata":{}},{"cell_type":"code","source":"session = run_data_explain(\n    file_bytes=file_bytes,\n    user_goal=\"Understand key patterns and trends in the dataset\",\n    user_id=\"richa\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:40.702933Z","iopub.execute_input":"2025-12-01T14:21:40.703176Z","iopub.status.idle":"2025-12-01T14:21:56.369595Z","shell.execute_reply.started":"2025-12-01T14:21:40.703148Z","shell.execute_reply":"2025-12-01T14:21:56.368847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ask_data(\"What is the average SepalLengthCm?\", file_bytes))\nprint(ask_data(\"How many unique Species are there?\", file_bytes))\nprint(ask_data(\"What percent of PetalWidthCm is missing?\", file_bytes))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:35:24.714413Z","iopub.execute_input":"2025-12-01T14:35:24.715183Z","iopub.status.idle":"2025-12-01T14:35:29.084529Z","shell.execute_reply.started":"2025-12-01T14:35:24.715155Z","shell.execute_reply":"2025-12-01T14:35:29.083763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 9: Show the Final Report","metadata":{}},{"cell_type":"code","source":"from IPython.display import Markdown, display\n\ndisplay(Markdown(session[\"final_report\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:32:57.425718Z","iopub.execute_input":"2025-12-01T14:32:57.426392Z","iopub.status.idle":"2025-12-01T14:32:57.432978Z","shell.execute_reply.started":"2025-12-01T14:32:57.426366Z","shell.execute_reply":"2025-12-01T14:32:57.431895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 10: Preview Generated Plots","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# -----------------------\n# Display Histograms\n# -----------------------\nprint(\"### HISTOGRAMS ###\")\nfor item in session[\"plots\"][\"histograms\"]:\n    img = mpimg.imread(item[\"file\"])     # use \"file\"\n    plt.figure(figsize=(5,4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(f\"Histogram: {item['column']}\")\n    plt.show()\n\n# -----------------------\n# Display Box Plots\n# -----------------------\nprint(\"### BOXPLOTS ###\")\nfor item in session[\"plots\"][\"boxplots\"]:\n    img = mpimg.imread(item[\"file\"])\n    plt.figure(figsize=(5,4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(f\"Box Plot: {item['column']}\")\n    plt.show()\n\n# -----------------------\n# Display Bar Plots\n# -----------------------\nprint(\"### BAR PLOTS ###\")\nfor item in session[\"plots\"][\"barplots\"]:\n    img = mpimg.imread(item[\"file\"])\n    plt.figure(figsize=(5,4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(f\"Bar Plot: {item['column']}\")\n    plt.show()\n\n# -----------------------\n# Display Heatmap\n# -----------------------\nprint(\"### HEATMAP ###\")\nif session[\"plots\"][\"heatmap\"]:\n    img = mpimg.imread(session[\"plots\"][\"heatmap\"])\n    plt.figure(figsize=(6,5))\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(\"Correlation Heatmap\")\n    plt.show()\nelse:\n    print(\"No numeric features â†’ No heatmap generated.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:21:58.925018Z","iopub.execute_input":"2025-12-01T14:21:58.925406Z","iopub.status.idle":"2025-12-01T14:22:00.544461Z","shell.execute_reply.started":"2025-12-01T14:21:58.925374Z","shell.execute_reply":"2025-12-01T14:22:00.543673Z"}},"outputs":[],"execution_count":null}]}